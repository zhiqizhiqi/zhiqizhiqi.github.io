<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>zhiqizhiqi</title><meta name=keywords content><meta name=description content=" - zhiqizhiqi"><meta name=author content="zhiqizhiqi"><link rel=canonical href=http://zhiqizhiqi.github.io/posts/newletters/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><link rel=icon href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://zhiqizhiqi.github.io/posts/newletters/index.xml><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="http://zhiqizhiqi.github.io/posts/newletters/"><meta property="og:image" content="http://zhiqizhiqi.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://zhiqizhiqi.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"http://zhiqizhiqi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"","item":"http://zhiqizhiqi.github.io/posts/newletters/"}]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://zhiqizhiqi.github.io/ accesskey=h title="Home (Alt + H)"><img src=http://zhiqizhiqi.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://zhiqizhiqi.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=http://zhiqizhiqi.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2>Week-AI-03：#<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h2></header><div class=entry-content><p>偷懒将近两个月，回来写点东西
0. TakeAway & 思考 1. 前沿技术追踪</p></div><footer class=entry-footer><span title='2023-05-22 11:30:03 +0000 UTC'>May 22, 2023</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;7 words&nbsp;·&nbsp;zhiqizhiqi</footer><a class=entry-link aria-label="post link to Week-AI-03：#" href=http://zhiqizhiqi.github.io/posts/newletters/weekly-230523/></a></article><article class=post-entry><header class=entry-header><h2>Week-AI-02：Pytorch2.0/USM</h2></header><div class=entry-content><p>0. TakeAway & 思考 1. 前沿技术追踪 1.1 Pytorch 2.0：全球免费获赠 1 倍算力！ 引入 torch.compile 这个大坑（包含了若干底层技术），DL 开发者以很低的开发成本享受计算效率（榨干硬件）的提升，可以预见未来很长一段时间，整个技术栈仍然会快速发展。另外，针对 transformer 的优化也是值得注意的，尤其是 MHA pytorch2.0 实现，包含了 FlashAttention 和 xFormers 等第三方的实现，开发者可以通过 pytorch 2.0 接口直接支持各种高度优化的 MHA 实现
torch.compile
效果：163 个开源模型 93% 可以直接调用 torch.compile，在 A100 上快 43%；FP32 精度下，快 21%，混合精度下，快 51% —> 免费的软件午餐，今年应该不需要买新机器了吧 “From day one, we knew the performance limits of eager execution” and “Our key criteria was to preserve certain kinds of flexibility – support for dynamic shapes and dynamic programs which researchers use in various stages of exploration....</p></div><footer class=entry-footer><span title='2023-03-20 11:30:03 +0000 UTC'>March 20, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;427 words&nbsp;·&nbsp;zhiqizhiqi</footer><a class=entry-link aria-label="post link to Week-AI-02：Pytorch2.0/USM" href=http://zhiqizhiqi.github.io/posts/newletters/weekly-230321/></a></article><article class=post-entry><header class=entry-header><h2>Week-AI-01：LLaMA; ViT 22B; Robo LLM(PaLM-E = ViT22B + PaLM 560B); Image RLHF;</h2></header><div class=entry-content><p>0. TakeAway & 思考 图像生成领域的可控性要求逐步提升，这也是目前学术界和工业界的研究主线，但缺少一个统一的框架来建模所有对图片潜在的操作，这个框架的收敛将是整个技术栈收敛的关键，这使得各种可控性能够任意组合，满足所有潜在的需求（比如我希望根据 prompt 生成与某个特定图片风格一致，和另外一个图片 edge detection 一致的图片时就无法实现） LLaMA 开源会加速 LLM 应用的研究和探索，期待大模型基础上可以玩出怎样的花样。同时，LLM 的开源生态在商业上会产生怎样的影响也是一个值得创业者思考的问题，“羊毛出在猪身上”，LLM 开源的“猪”是什么呢？ 图像领域的 proxy task 应该是什么？使用 JFT 训练是更好的选择么？ 还比较值得考虑的是 Mulit-Modal 之间的互监督以及 Masked Autoencoder。前者来自于数据自然呈现的结果更加 scalable（而依赖 JFT 势必更加依赖离线 auto-label 流程以及可能的人工标注，都在可扩展性上不够好），后者能够捕捉数据内在的结构，对于表征学习来说也很重要。 从框架的可扩展性和要素成熟度来看，图片相对于文本明显还有更长的道路要走 越来越明显地一个趋势是让机器先将人类世界现存的所有数据都先"learn"一遍，以此快速提升模型的通用性，而这个模型也将成为所有 AI 系统的必要组件。 对多模态来说，ground to 物理世界是最重要的突破点，将通用智能从虚拟世界带到物理世界。而物理世界的智能体可能更加是“国家机器”关注的形态，也是必争之地。如果看三年后创投圈什么机会将再次出现，很有可能是通用物理机器人。 1. 前沿技术追踪 1.1 LLaMA：开源 LLM，撬动全球零散算力和脑力资源的杠杆 Meta 汇总 GPT-3 之后近两年所有重要的 LLM 改进，训练了 LLM 模型，规模从 7B 到 65B，在大量 NLP Task 的评测上大部分比 GPT-3 & Chinchilla-70B & PaLM-540B 更好，更重要的是此项工作完成基于公开数据，并且模型会进行开源分享！
如何实现的？
数据：除了广泛被使用的 CommonCrawl & C4 & Wikipedia & Book 之外，还直接使用了 Github & Arxiv & StackExchange，分别对应代码、（各个学科）科学论文和高质量的问答数据。注：这些改动为此项工作后续的实验结果解读增加了很多干扰 网络结构：在原始 transformer 上引入了 pre-normalization（提升训练稳定性）、SwiGLU（提升性能）、Rotary Embeddings（position embedding 改进）。 训练加速：针对 MHA（multi-head attention）实现了更高效的代码（注：xformers 内有对应实现）减少内存和计算；若干 common 操作：forward activation checkpoint（细节在于手动指定层，而不是自动计算的结果，更加快） & 并行 & 计算和传输并行等。 训练效率：在 2048 块 A100 卡上训练 65B 模型，380 tokens/sec/GPU，21 天训练了 1....</p></div><footer class=entry-footer><span title='2023-03-11 15:30:03 +0000 UTC'>March 11, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;617 words&nbsp;·&nbsp;zhiqizhiqi</footer><a class=entry-link aria-label="post link to Week-AI-01：LLaMA; ViT 22B; Robo LLM(PaLM-E = ViT22B + PaLM 560B); Image RLHF;" href=http://zhiqizhiqi.github.io/posts/newletters/weekly-230304/></a></article><article class=post-entry><header class=entry-header><h2>Week-AI-00：ToolFormer; ControlNet</h2></header><div class=entry-content><p>1. 前沿技术追踪 1.1 ToolFormer： 训练 LLM 使用工具，机器智能发生从猿到人的转变 Meta 的研究人员训练 LLM 去使用 API（a calculator, a Q&amp;A system, a search engine, a translation system, and a calendar），从结果来看，明显补充了 LLM 部分能力的不足。更重要的是，他们给出了一个通用的方法来让模型学习使用任意的工具（API）。从实验结果来看，6.7B GPT-J 模型 with ToolFormer 可以超越 66B OPT 和 175B GPT-3 的算法性能
什么是 ToolFormer “a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.”
内部是如何实现的？ 数据生成的方式：从 plain text 出发，使用 in context 的方式让 LLM 生成若干潜在有用的 API 候选，根据 perplexity 来找出其中有用的 API 候选。将筛选后的 API 融合在原始数据中对模型进行 finetune。若干数据样例...</p></div><footer class=entry-footer><span title='2023-02-24 11:30:03 +0000 UTC'>February 24, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;387 words&nbsp;·&nbsp;zhiqizhiqi</footer><a class=entry-link aria-label="post link to Week-AI-00：ToolFormer; ControlNet" href=http://zhiqizhiqi.github.io/posts/newletters/weekly-230224/></a></article></main><footer class=footer><span>&copy; 2023 <a href=http://zhiqizhiqi.github.io/>zhiqizhiqi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>